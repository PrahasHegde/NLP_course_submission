import sys
import time
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.markdown import Markdown
from rich.align import Align
from llama_cpp import Llama

# --- AESTHETICS CONFIGURATION ---
console = Console()

STYLE_BORDER = "cyan"
STYLE_POEM = "white on grey11"
STYLE_PROMPT = "bold green"

def load_model(model_path):
    """Load the TinyLlama model."""
    with console.status("[bold magenta]Awaking the Muse... (Loading TinyLlama)[/]", spinner="dots"):
        try:
            # TinyLlama is small, so we can use a smaller context window (n_ctx=1024) safely
            llm = Llama(model_path=model_path, n_ctx=2048, verbose=False)
            return llm
        except Exception as e:
            console.print(f"[bold red]Error loading model:[/]\n{e}")
            sys.exit(1)

def generate_poem(llm, topic):
    """Prompt TinyLlama to write poetry."""
    
    system_prompt = (
        "You are a soulful poet. Write a poem about the user's topic. "
        "Do not explain the poem. Just write the raw poetry."
    )
    
    # TinyLlama specific prompt format (<|system|>, <|user|>, etc.)
    full_prompt = (
        f"<|system|>\n{system_prompt}</s>\n"
        f"<|user|>\nTopic: {topic}</s>\n"
        f"<|assistant|>\n"
    )

    stream = llm(
        full_prompt, 
        max_tokens=256, 
        stop=["</s>", "<|user|>"], # Stop generating when it tries to speak as user
        stream=True, 
        temperature=0.8,
        top_p=0.9
    )
    
    return stream

# --- MAIN APP LOOP ---
def main():
    console.clear()
    console.rule("[bold magenta] PoetBot[/]")
    console.print(Align.center("[italic dim]An Offline AI Poetry Generator[/]"))
    console.print("\n")

    # UPDATED FILENAME HERE
    MODEL_PATH = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
    
    # Check if file exists before crashing
    import os
    if not os.path.exists(MODEL_PATH):
        console.print(f"[bold red]Error:[/] Could not find '{MODEL_PATH}'")
        console.print("Please make sure you downloaded the file and put it in this folder.")
        return

    llm = load_model(MODEL_PATH)
    
    console.print(Panel("The Muse is listening...", style=STYLE_BORDER, expand=False))

    while True:
        console.print(f"\n[{STYLE_PROMPT}]Give me a theme (or 'q' to quit):[/{STYLE_PROMPT}] ", end="")
        topic = input()
        
        if topic.lower() in ['q', 'quit', 'exit']:
            console.print("\n[magenta]The Muse sleeps.[/]")
            break

        if not topic.strip():
            continue

        console.print("\n")
        
        poem_content = ""
        panel = Panel(poem_content, title="[italic]Writing...[/]", border_style=STYLE_BORDER, padding=(1, 2))
        
        with Live(panel, refresh_per_second=10, console=console) as live:
            stream = generate_poem(llm, topic)
            
            for output in stream:
                token = output['choices'][0]['text']
                poem_content += token
                live.update(Panel(
                    Markdown(poem_content, justify="center"), 
                    title=f"[bold magenta]{topic.upper()}[/]",
                    border_style=STYLE_BORDER,
                    padding=(1, 2)
                ))

        console.print(Align.right("[dim]-- generated by TinyLlama --[/]"))
        console.print("\n")

if __name__ == "__main__":
    main()